{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1af256-6245-4d02-9754-c82ab545a343",
   "metadata": {},
   "source": [
    "# PyTorch 神经网络基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f1291-49da-4de6-8472-2041820bee6e",
   "metadata": {},
   "source": [
    "神经网络是一种模仿人脑处理信息方式的计算模型，它由许多相互连接的节点（神经元）组成，这些节点按层次排列。\n",
    "\n",
    "神经网络的强大之处在于其能够自动从大量数据中学习复杂的模式和特征，无需人工设计特征提取器。\n",
    "\n",
    "随着深度学习的发展，神经网络已经成为解决许多复杂问题的关键技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49531ff-46ec-4a3a-915e-59eecebd26e4",
   "metadata": {},
   "source": [
    "**神经元（Neuron）**\n",
    "\n",
    "神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，然后通过激活函数处理以产生输出。\n",
    "\n",
    "神经元的权重和偏置是网络学习过程中需要调整的参数。\n",
    "\n",
    "**输入和输出**:\n",
    "\n",
    "- **输入（Input）**：输入是网络的起始点，可以是特征数据，如图像的像素值或文本的词向量。\n",
    "\n",
    "- **输出（Output）**：输出是网络的终点，表示模型的预测结果，如分类任务中的类别标签。\n",
    "\n",
    "神经元接收多个输入（例如x1, x2, ..., xn），如果输入的加权和大于激活阈值（activation potential），则产生二进制输出。\n",
    "\n",
    "神经元的输出可以看作是输入的加权和加上偏置（bias）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1948d7c-79b0-4a46-8a97-4a13e6be755a",
   "metadata": {},
   "source": [
    "**层（Layer）**\n",
    "\n",
    "输入层和输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置。\n",
    "\n",
    "神经网络由多个层组成，包括：\n",
    "\n",
    "- **输入层（Input Layer）**：接收原始输入数据。\n",
    "\n",
    "- **隐藏层（Hidden Layer）**：对输入数据进行处理，可以有多个隐藏层。\n",
    "\n",
    "- **输出层（Output Layer）**：产生最终的输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964240d-a699-46f2-956b-25eb9205beae",
   "metadata": {},
   "source": [
    "**前馈神经网络（Feedforward Neural Network，FNN）**\n",
    "\n",
    "前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元。\n",
    "\n",
    "前馈神经网络特点是数据从输入层开始，经过一个或多个隐藏层，最后到达输出层，全过程没有循环或反馈。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d7694-2caf-4d95-8836-6661c3cb0a69",
   "metadata": {},
   "source": [
    "**前馈神经网络的基本结构**：\n",
    "\n",
    "- **输入层**： 数据进入网络的入口点。输入层的每个节点代表一个输入特征。\n",
    "\n",
    "- **隐藏层**：一个或多个层，用于捕获数据的非线性特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力。\n",
    "\n",
    "- **输出层**：输出网络的预测结果。节点数和问题类型相关，例如分类问题的输出节点数等于类别数。\n",
    "\n",
    "- **连接权重与偏置**：每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801fe58-af60-43bb-a18c-95c16f86e1c4",
   "metadata": {},
   "source": [
    "**循环神经网络（Recurrent Neural Network, RNN）**\n",
    "\n",
    "循环神经网络（Recurrent Neural Network, RNN）络是一类专门处理序列数据的神经网络，能够捕获输入数据中时间或顺序信息的依赖关系。\n",
    "\n",
    "RNN 的特别之处在于它具有\"记忆能力\"，可以在网络的隐藏状态中保存之前时间步的信息。\n",
    "\n",
    "循环神经网络用于处理随时间变化的数据模式。\n",
    "\n",
    "在 RNN 中，相同的层被用来接收输入参数，并在指定的神经网络中显示输出参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4eadd-279a-4d99-b57f-d71457c15f91",
   "metadata": {},
   "source": [
    "PyTorch 提供了强大的工具来构建和训练神经网络。\n",
    "\n",
    "神经网络在 PyTorch 中是通过 torch.nn 模块来实现的。\n",
    "\n",
    "torch.nn 模块提供了各种网络层（如全连接层、卷积层等）、损失函数和优化器，让神经网络的构建和训练变得更加方便。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814e207-6e34-459c-b12c-b3ec15b48c0e",
   "metadata": {},
   "source": [
    "在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类。\n",
    "\n",
    "nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：\n",
    "\n",
    "- \\__init__()：定义网络层。\n",
    "\n",
    "- forward()：定义数据的前向传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc7c7f-625b-47a0-a86a-e3c2d518801b",
   "metadata": {},
   "source": [
    "简单的全连接神经网络（Fully Connected Network）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840ee975-3ca2-4fa6-b97f-6a2bc1174027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T15:17:29.062558Z",
     "iopub.status.busy": "2024-12-27T15:17:29.062170Z",
     "iopub.status.idle": "2024-12-27T15:17:30.138926Z",
     "shell.execute_reply": "2024-12-27T15:17:30.138438Z",
     "shell.execute_reply.started": "2024-12-27T15:17:29.062532Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 定义一个输入层到隐藏层的全连接层\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入 2 个特征，输出 2 个特征\n",
    "        # 定义一个隐藏层到输出层的全连接层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 输入 2 个特征，输出 1 个预测值\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播过程\n",
    "        x = torch.relu(self.fc1(x))  # 使用 ReLU 激活函数\n",
    "        x = self.fc2(x)  # 输出层\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf7d196-30ab-4fdf-88fe-edf82f014cd9",
   "metadata": {},
   "source": [
    "PyTorch 提供了许多常见的神经网络层，以下是几个常见的：\n",
    "\n",
    "- nn.Linear(in_features, out_features)：全连接层，输入 in_features 个特征，输出 out_features 个特征。\n",
    "\n",
    "- nn.Conv2d(in_channels, out_channels, kernel_size)：2D 卷积层，用于图像处理。\n",
    "\n",
    "- nn.MaxPool2d(kernel_size)：2D 最大池化层，用于降维。\n",
    "\n",
    "- nn.ReLU()：ReLU 激活函数，常用于隐藏层。\n",
    "\n",
    "- nn.Softmax(dim)：Softmax 激活函数，通常用于输出层，适用于多类分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b736eb4-1d2f-4da3-9672-6a3ea88f29b8",
   "metadata": {},
   "source": [
    "**激活函数（Activation Function）**\n",
    "\n",
    "激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：\n",
    "\n",
    "- Sigmoid：用于二分类问题，输出值在 0 和 1 之间。\n",
    "\n",
    "- Tanh：输出值在 -1 和 1 之间，常用于输出层之前。\n",
    "\n",
    "- ReLU（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 f(x) = max(0, x)，有助于解决梯度消失问题。\n",
    "\n",
    "- Softmax：常用于多分类问题的输出层，将输出转换为概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab67f2-caeb-48b9-a065-750c518edab3",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ReLU 激活\n",
    "output = F.relu(input_tensor)\n",
    "\n",
    "# Sigmoid 激活\n",
    "output = torch.sigmoid(input_tensor)\n",
    "\n",
    "# Tanh 激活\n",
    "output = torch.tanh(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67beddf-f53e-450f-8aea-0e9bd8f6c6dd",
   "metadata": {},
   "source": [
    "**损失函数（Loss Function）**\n",
    "\n",
    "损失函数用于衡量模型的预测值与真实值之间的差异。\n",
    "\n",
    "常见的损失函数包括：\n",
    "\n",
    "- 均方误差（MSELoss）：回归问题常用，计算输出与目标值的平方差。\n",
    "\n",
    "- 交叉熵损失（CrossEntropyLoss）：分类问题常用，计算输出和真实标签之间的交叉熵。\n",
    "\n",
    "- BCEWithLogitsLoss：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b327c4c-4358-4e4b-90ff-0063295e8149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 均方误差损失\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 二分类交叉熵损失\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594eccc-e07a-4c7d-9f2c-092efdd15e26",
   "metadata": {},
   "source": [
    "**优化器（Optimizer）**\n",
    "\n",
    "优化器负责在训练过程中更新网络的权重和偏置。\n",
    "\n",
    "常见的优化器包括：\n",
    "\n",
    "- SGD（随机梯度下降）\n",
    "\n",
    "- Adam（自适应矩估计）\n",
    "\n",
    "- RMSprop（均方根传播）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abf92a-d4fa-49f1-87dc-502aafcb2431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 使用 SGD 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 使用 Adam 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c90f61-6941-4b29-9ed2-801d69976019",
   "metadata": {},
   "source": [
    "**训练过程（Training Process）**\n",
    "\n",
    "训练神经网络涉及以下步骤：  \n",
    "\n",
    "**1.准备数据**：通过 DataLoader 加载数据。\n",
    "    \n",
    "**2.定义损失函数和优化器**。    \n",
    "    \n",
    "**3.前向传播**：计算模型的输出。   \n",
    "    \n",
    "**4.计算损失**：与目标进行比较，得到损失值。  \n",
    "    \n",
    "**5.反向传播**：通过 loss.backward() 计算梯度。    \n",
    "    \n",
    "**6.更新参数**：通过 optimizer.step() 更新模型的参数。   \n",
    "    \n",
    "**7.重复上述步骤**，直到达到预定的训练轮数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71e8e3-c140-46b3-ad3d-1837c8e45459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假设已经定义好了模型、损失函数和优化器\n",
    "\n",
    "# 训练数据示例\n",
    "X = torch.randn(10, 2)  # 10 个样本，每个样本有 2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标标签\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    optimizer.zero_grad()  # 清除梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新权重\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:  # 每 10 轮输出一次损失\n",
    "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14a353-fc40-437d-8351-d4ce1c206867",
   "metadata": {},
   "source": [
    "**测试与评估**\n",
    "\n",
    "训练完成后，需要对模型进行测试和评估。\n",
    "\n",
    "常见的步骤包括：\n",
    "\n",
    "- **计算测试集的损失**：测试模型在未见过的数据上的表现。\n",
    "\n",
    "- **计算准确率（Accuracy）**：对于分类问题，计算正确预测的比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413b0b5-302b-41bb-89ef-cdda97e748c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你有测试集 X_test 和 Y_test\n",
    "model.eval()  # 设置模型为评估模式\n",
    "with torch.no_grad():  # 在评估过程中禁用梯度计算\n",
    "    output = model(X_test)\n",
    "    loss = criterion(output, Y_test)\n",
    "    print(f'Test Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994275e-c4a8-4470-ad42-bed2662910db",
   "metadata": {},
   "source": [
    "**神经网络类型**\n",
    "\n",
    "    1.前馈神经网络（Feedforward Neural Networks）：数据单向流动，从输入层到输出层，无反馈连接。\n",
    "\n",
    "    2.卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像处理，使用卷积层提取空间特征。\n",
    "\n",
    "    3.循环神经网络（Recurrent Neural Networks, RNNs）：适用于序列数据，如时间序列分析和自然语言处理，允许信息反馈循环。\n",
    "\n",
    "    4.长短期记忆网络（Long Short-Term Memory, LSTM）：一种特殊的RNN，能够学习长期依赖关系。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
