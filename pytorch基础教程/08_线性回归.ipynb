{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 线性回归\n",
    "\n",
    "线性回归是最基本的机器学习算法之一，用于预测一个连续值。\n",
    "\n",
    "线性回归是一种简单且常见的回归分析方法，目的是通过拟合一个线性函数来预测输出。\n",
    "\n",
    "对于一个简单的线性回归问题，模型可以表示为：Y = W1X1 + W2X2 + W3X3 + ... + WnXn + b\n",
    "\n",
    "- y 是预测值（目标值）。\n",
    "\n",
    "- x1，x2，xn 是输入特征。\n",
    "\n",
    "- w1，w2，wn是待学习的权重（模型参数）。\n",
    "\n",
    "- b 是偏置项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，线性回归模型可以通过继承 nn.Module 类来实现。我们将通过一个简单的示例来详细说明如何使用 PyTorch 实现线性回归模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备 \n",
    "\n",
    "我们首先准备一些假数据，用于训练我们的线性回归模型。这里，我们可以生成一个简单的线性关系的数据集，其中每个样本有两个特征 x1，x2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873],\n",
      "        [ 0.9007, -2.1055],\n",
      "        [ 0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047],\n",
      "        [-0.7521,  1.6487]])\n",
      "tensor([12.4460, -0.4663,  1.7666, -0.9357,  7.4781])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 随机种子，确保每次运行结果一致\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 生成训练数据\n",
    "X = torch.randn(100, 2)  # 100 个样本，每个样本 2 个特征\n",
    "true_w = torch.tensor([2.0, 3.0])  # 假设真实权重\n",
    "true_b = 4.0  # 偏置项\n",
    "Y = X @ true_w + true_b + torch.randn(100) * 0.1  # 加入一些噪声\n",
    "\n",
    "# 打印部分数据\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码创建了一个带有噪声的线性数据集，输入 X 为 100x2 的矩阵，每个样本有两个特征，输出 Y 由真实的权重和偏置生成，并加上了一些随机噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义线性回归模型\n",
    "\n",
    "我们可以通过继承 nn.Module 来定义一个简单的线性回归模型。在 PyTorch 中，线性回归的核心是 nn.Linear() 层，它会自动处理权重和偏置的初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 定义线性回归模型\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 定义一个线性层，输入为2个特征，输出为1个预测值\n",
    "        self.linear = nn.Linear(2, 1)  # 输入维度2，输出维度1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # 前向传播，返回预测结果\n",
    "\n",
    "\n",
    "# 创建模型实例\n",
    "model = LinearRegressionModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 nn.Linear(2, 1) 表示一个线性层，它有 2 个输入特征和 1 个输出。forward 方法定义了如何通过这个层进行前向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义损失函数与优化器\n",
    "\n",
    "线性回归的常见损失函数是 均方误差损失（MSELoss），用于衡量预测值与真实值之间的差异。PyTorch 中提供了现成的 MSELoss 函数。\n",
    "\n",
    "我们将使用 SGD（随机梯度下降） 或 Adam 优化器来最小化损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数（均方误差）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 优化器（使用 SGD 或 Adam）\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 学习率设置为0.01\n",
    "\n",
    "\n",
    "#MSELoss：计算预测值与真实值的均方误差。\n",
    "\n",
    "#SGD：使用随机梯度下降法更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "在训练过程中，我们将执行以下步骤：\n",
    "\n",
    "1. 使用输入数据 X 进行前向传播，得到预测值。\n",
    "\n",
    "2. 计算损失（预测值与实际值之间的差异）。\n",
    "\n",
    "3. 使用反向传播计算梯度。\n",
    "\n",
    "4. 更新模型参数（权重和偏置）。\n",
    "\n",
    "我们将训练模型 1000 轮，并在每 100 轮打印一次损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "num_epochs = 1000  # 训练 1000 轮\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "\n",
    "    # 前向传播\n",
    "    predictions = model(X)  # 模型输出预测值\n",
    "    loss = criterion(predictions.squeeze(), Y)  # 计算损失（注意预测值需要压缩为1D）\n",
    "\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    loss.backward()  # 计算梯度\n",
    "    optimizer.step()  # 更新模型参数\n",
    "\n",
    "    # 打印损失\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/1000], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predictions.squeeze()：我们在这里将模型的输出从 2D 张量压缩为 1D，因为目标值 Y 是一个一维数组。\n",
    "\n",
    "- optimizer.zero_grad()：每次反向传播前需要清空之前的梯度。\n",
    "\n",
    "- loss.backward()：计算梯度。\n",
    "\n",
    "- optimizer.step()：更新权重和偏置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型\n",
    "\n",
    "训练完成后，我们可以通过查看模型的权重和偏置来评估模型的效果。我们还可以在新的数据上进行预测并与实际值进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练后的权重和偏置\n",
    "print(f\"Predicted weight: {model.linear.weight.data.numpy()}\")\n",
    "print(f\"Predicted bias: {model.linear.bias.data.numpy()}\")\n",
    "\n",
    "# 在新数据上做预测\n",
    "with torch.no_grad():  # 评估时不需要计算梯度\n",
    "    predictions = model(X)\n",
    "\n",
    "# 可视化预测与实际值\n",
    "plt.scatter(X[:, 0], Y, color=\"blue\", label=\"True values\")\n",
    "plt.scatter(X[:, 0], predictions, color=\"red\", label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.linear.weight.data 和 model.linear.bias.data：这些属性存储了模型的权重和偏置。\n",
    "\n",
    "- torch.no_grad()：在评估模式下，不需要计算梯度，节省内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果分析\n",
    "\n",
    "在训练过程中，随着损失逐渐减小，我们希望最终的模型能够拟合我们生成的数据。通过查看训练后的权重和偏置，我们可以比较其与真实值（true_w 和 true_b）的差异。理论上，模型的输出权重应该接近 true_w 和 true_b。\n",
    "\n",
    "在可视化的散点图中，蓝色点表示真实值，红色点表示模型的预测值。我们希望看到红色点与蓝色点尽可能接近，表明模型成功学习了数据的线性关系。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
