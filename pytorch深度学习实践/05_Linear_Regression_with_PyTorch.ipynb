{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e73f394-1f45-4d6b-8467-215fa361362e",
   "metadata": {},
   "source": [
    "PyTorch Fashion(风格)\n",
    "\n",
    "1、prepare dataset\n",
    "\n",
    "2、design model using Class  # 目的是为了前向传播forward，即计算y hat(预测值)\n",
    "\n",
    "3、Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。\n",
    "\n",
    "4、Training cycle (forward,backward,update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384af5e-b2fb-416a-a2d2-09ee69754322",
   "metadata": {},
   "source": [
    "每一次epoch的训练过程，总结就是：\n",
    "\n",
    "①前向传播，求y hat （输入的预测值）\n",
    "\n",
    "②根据y_hat和y_label(y_data)计算loss\n",
    "\n",
    "③反向传播 backward (计算梯度)\n",
    "\n",
    "④根据梯度，更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7d195-032d-40de-9813-d57c049ce708",
   "metadata": {},
   "source": [
    "**注意：**  \n",
    "本实例是批量数据处理，小伙伴们不要被optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)误导了，以为见了SGD就是随机梯度下降。要看传进来的数据是单个的还是批量的。这里的x_data是3个数据，是一个batch，调用的PyTorch API是 torch.optim.SGD，但这里的SGD不是随机梯度下降，而是批量梯度下降。也就是说，梯度下降算法使用的是随机梯度下降，还是批量梯度下降，还是mini-batch梯度下降，用的API都是 torch.optim.SGD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6698e33-39ac-4852-97fd-b12ae4fdd7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:49:23.269723Z",
     "iopub.status.busy": "2024-12-15T14:49:23.269409Z",
     "iopub.status.idle": "2024-12-15T14:49:24.832855Z",
     "shell.execute_reply": "2024-12-15T14:49:24.832365Z",
     "shell.execute_reply.started": "2024-12-15T14:49:23.269703Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50.20075988769531\n",
      "1 22.35279655456543\n",
      "2 9.95560359954834\n",
      "3 4.436656951904297\n",
      "4 1.9797122478485107\n",
      "5 0.8858824968338013\n",
      "6 0.3988749086856842\n",
      "7 0.18200787901878357\n",
      "8 0.08540153503417969\n",
      "9 0.04233196750283241\n",
      "10 0.02309659868478775\n",
      "11 0.014472602866590023\n",
      "12 0.010573122650384903\n",
      "13 0.008777813985943794\n",
      "14 0.00792006216943264\n",
      "15 0.007480614818632603\n",
      "16 0.007228126749396324\n",
      "17 0.007059725001454353\n",
      "18 0.006929495837539434\n",
      "19 0.00681713642552495\n",
      "20 0.006713441107422113\n",
      "21 0.006614430341869593\n",
      "22 0.006518272217363119\n",
      "23 0.006424075923860073\n",
      "24 0.0063315341249108315\n",
      "25 0.006240433547645807\n",
      "26 0.00615069130435586\n",
      "27 0.006062323227524757\n",
      "28 0.005975137464702129\n",
      "29 0.005889252759516239\n",
      "30 0.005804664455354214\n",
      "31 0.005721216090023518\n",
      "32 0.005639031063765287\n",
      "33 0.0055579738691449165\n",
      "34 0.0054781148210167885\n",
      "35 0.005399354267865419\n",
      "36 0.00532177509739995\n",
      "37 0.005245291627943516\n",
      "38 0.005169855430722237\n",
      "39 0.0050955843180418015\n",
      "40 0.005022348836064339\n",
      "41 0.004950178787112236\n",
      "42 0.0048790317960083485\n",
      "43 0.004808931145817041\n",
      "44 0.004739790689200163\n",
      "45 0.004671663045883179\n",
      "46 0.0046045491471886635\n",
      "47 0.004538391251116991\n",
      "48 0.004473171196877956\n",
      "49 0.004408866632729769\n",
      "50 0.004345507360994816\n",
      "51 0.00428303238004446\n",
      "52 0.004221497569233179\n",
      "53 0.004160824231803417\n",
      "54 0.004101040307432413\n",
      "55 0.004042091779410839\n",
      "56 0.003983988426625729\n",
      "57 0.003926739562302828\n",
      "58 0.0038703102618455887\n",
      "59 0.0038146949373185635\n",
      "60 0.003759845858439803\n",
      "61 0.00370583264157176\n",
      "62 0.0036525651812553406\n",
      "63 0.003600084688514471\n",
      "64 0.0035483338870108128\n",
      "65 0.003497322089970112\n",
      "66 0.00344707234762609\n",
      "67 0.003397521562874317\n",
      "68 0.0033486962784081697\n",
      "69 0.003300598356872797\n",
      "70 0.0032531439792364836\n",
      "71 0.003206375055015087\n",
      "72 0.003160337219014764\n",
      "73 0.003114895196631551\n",
      "74 0.003070143284276128\n",
      "75 0.003026009304448962\n",
      "76 0.0029825186356902122\n",
      "77 0.002939632860943675\n",
      "78 0.002897398779168725\n",
      "79 0.0028557563200592995\n",
      "80 0.0028147143311798573\n",
      "81 0.002774266293272376\n",
      "82 0.002734406152740121\n",
      "83 0.0026951057370752096\n",
      "84 0.0026563662104308605\n",
      "85 0.0026182071305811405\n",
      "86 0.002580569125711918\n",
      "87 0.002543465234339237\n",
      "88 0.0025069438852369785\n",
      "89 0.0024709152057766914\n",
      "90 0.002435382455587387\n",
      "91 0.0024003919679671526\n",
      "92 0.002365881111472845\n",
      "93 0.0023318922612816095\n",
      "94 0.0022983779199421406\n",
      "95 0.0022653331980109215\n",
      "96 0.0022327969782054424\n",
      "97 0.0022007059305906296\n",
      "98 0.0021690642461180687\n",
      "99 0.002137885196134448\n",
      "w =  1.9692188501358032\n",
      "b =  0.06997289508581161\n",
      "y_pred =  tensor([[7.9468]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# prepare dataset\n",
    "# x,y是矩阵，3行1列 也就是说总共有3个数据，每个数据只有1个特征\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "#design model using class\n",
    "\n",
    "\"\"\"\n",
    "our model class should be inherit from nn.Module, which is base class for all neural network modules.\n",
    "member methods __init__() and forward() have to be implemented\n",
    "class nn.linear contain two member Tensors: weight and bias\n",
    "class nn.Linear has implemented the magic method __call__(),which enable the instance of the class can\n",
    "be called just like a function.Normally the forward() will be called \n",
    "\"\"\"\n",
    "\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # (1,1)是指输入x和输出y的特征维度，这里数据集中的x和y的特征都是1维的\n",
    "        # 该线性层需要学习的参数是w和b  获取w/b的方式分别是~linear.weight/linear.bias\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# construct loss and optimizer\n",
    "# criterion = torch.nn.MSELoss(size_average = False)\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)      # model.parameters()自动完成参数的初始化操作，这个地方我可能理解错了\n",
    "\n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)                # forward:predict\n",
    "    loss = criterion(y_pred, y_data)      # forward: loss\n",
    "    print(epoch, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()       # the grad computer by .backward() will be accumulated. so before backward, remember set the grad to zero\n",
    "    loss.backward()             # backward: autograd，自动计算梯度\n",
    "    optimizer.step()            # update 参数，即更新w和b的值\n",
    "\n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    "\n",
    "x_test = torch.tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
